{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Import"
      ],
      "metadata": {
        "id": "WZihchvPjE69"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjwP_dIVi9vR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "from torchvision.models import ResNet50_Weights\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, f1_score, recall_score,\n",
        "    precision_score, roc_auc_score, average_precision_score, accuracy_score\n",
        ")\n",
        "\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhIfwgdMjLK0",
        "outputId": "cc21bd94-99e6-4fb9-f2d0-e7be7fdc903e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "aqUH2iyujNAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data & Model loading"
      ],
      "metadata": {
        "id": "v51oonVIjoJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = Path(\"/content/drive/My Drive/졸업프로젝트/TestDataset/\")\n",
        "IMG_MODEL_PATH = BASE_DIR / \"MMOTU/resnet50_binary_mmotu_aug.pth\"\n",
        "MULTIMODAL_MODEL_PATH = BASE_DIR / \"multimodal_classifier_latefusion.pth\"\n",
        "\n",
        "# Clinical Data\n",
        "PROCESSED_CLIN_DIR = Path(BASE_DIR) / \"Processed_Clinical_Data\"\n",
        "DL_TRAIN_NPZ = PROCESSED_CLIN_DIR / 'dl_train_data.npz'\n",
        "DL_TEST_NPZ = PROCESSED_CLIN_DIR / 'dl_test_data.npz'\n",
        "\n",
        "# US image data\n",
        "PROCESSED_IMG_DIR = BASE_DIR / \"MMOTU/ResNet50_ViT_Processed_Data\"\n",
        "TRAIN_IMG_NPZ = PROCESSED_IMG_DIR / \"train_augmented2_data.npz\"\n",
        "TEST_IMG_NPZ = PROCESSED_IMG_DIR / \"validation_data.npz\""
      ],
      "metadata": {
        "id": "pbKVogIsjOOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Preprocessing"
      ],
      "metadata": {
        "id": "A5yInRRHj3js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BINARY_MAPPING = {\n",
        "    0: 0, # Chocolate cyst (Benign)\n",
        "    1: 0, # Serous cystadenoma (Benign)\n",
        "    2: 0, # Teratoma (Benign)\n",
        "    3: 0, # Theca cell tumor (Benign)\n",
        "    4: 0, # Simple cyst (Benign)\n",
        "    5: 0, # Normal ovary (Benign)\n",
        "    6: 0, # Mucinous cystadenoma (Benign/Borderline)\n",
        "    7: 1  # High grade serous cystadenocarcinoma (Malignant)\n",
        "}"
      ],
      "metadata": {
        "id": "NhY5I08ljugb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MMOTUUnpairedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    초음파 이미지 데이터(.npz)와 임상 데이터(.npz)를 독립적으로 loading하고,\n",
        "    Co-training을 위한 random sampling 수행\n",
        "    \"\"\"\n",
        "    def __init__(self, img_npz_path, X_clin_np, Y_clin_np, device='cpu'):\n",
        "        img_data = np.load(img_npz_path, allow_pickle=True)\n",
        "        self.img_images = torch.from_numpy(img_data['images']).float()\n",
        "        self.img_labels = torch.from_numpy(img_data['labels'].astype(np.float32)).float()\n",
        "\n",
        "        self.clin_data = torch.from_numpy(X_clin_np).float()\n",
        "        self.clin_labels = torch.from_numpy(Y_clin_np).float()\n",
        "\n",
        "        self.img_len = len(self.img_images)\n",
        "        self.clin_len = len(self.clin_data)\n",
        "        self.max_len = max(self.img_len, self.clin_len)\n",
        "        print(f\"Dataset Initialized. Image Samples: {self.img_len}, Clinical Samples: {self.clin_len}\")\n",
        "\n",
        "    def __len__(self): return self.max_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_idx = idx % self.img_len\n",
        "        clin_idx = random.randint(0, self.clin_len - 1)\n",
        "\n",
        "        return (\n",
        "            self.img_images[img_idx],\n",
        "            self.clin_data[clin_idx],\n",
        "            self.img_labels[img_idx],\n",
        "            self.clin_labels[clin_idx]\n",
        "        )"
      ],
      "metadata": {
        "id": "BGFsbW2AofH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Encoders"
      ],
      "metadata": {
        "id": "jasEC-GLtkrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, model_path, device):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        self.backbone = nn.Sequential(\n",
        "            resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool,\n",
        "            resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4,\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        self.output_dim = resnet.fc.in_features  # 2048\n",
        "\n",
        "        try:\n",
        "            state_dict = torch.load(model_path, map_location='cpu')\n",
        "            model_dict = self.backbone.state_dict()\n",
        "            pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
        "            model_dict.update(pretrained_dict)\n",
        "            self.backbone.load_state_dict(model_dict, strict=False)\n",
        "            print(f\"Image Encoder loaded successfully from {model_path.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN]: Failed to load pretrained Image Encoder. Training from scratch. Error: {e}\")\n",
        "\n",
        "        # fine tuning : layer 3, 4만 학습 가능하도록 설정\n",
        "        for name, param in self.backbone.named_parameters():\n",
        "             if 'layer4' in name or 'layer3' in name:\n",
        "                 param.requires_grad = True\n",
        "             else:\n",
        "                 param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.flatten(self.backbone(x), 1)"
      ],
      "metadata": {
        "id": "PfA8VaYisw4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiModalClassifier_LateFusion(nn.Module):\n",
        "    def __init__(self, img_encoder, clinical_dim, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        # Image Encoder\n",
        "        self.image_encoder = img_encoder\n",
        "        self.image_feature_dim = img_encoder.output_dim\n",
        "\n",
        "        # Clinical Encoder\n",
        "        self.clinical_encoder = nn.Sequential(\n",
        "            nn.Linear(clinical_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.clinical_feature_dim = 128\n",
        "\n",
        "        # Modality-specific Heads\n",
        "        self.image_head = nn.Linear(self.image_feature_dim, 1)\n",
        "        self.clinical_head = nn.Linear(128, 1)\n",
        "\n",
        "        # Fusion Head\n",
        "        self.fusion_proj = nn.Sequential(\n",
        "            nn.Linear(self.image_feature_dim + 128, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "        self.fusion_head = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, x_img, x_clin):\n",
        "        img_feat = self.image_encoder(x_img)\n",
        "        clin_feat = self.clinical_encoder(x_clin)\n",
        "\n",
        "        img_out = self.image_head(img_feat)\n",
        "        clin_out = self.clinical_head(clin_feat)\n",
        "\n",
        "        fused = torch.cat([img_feat, clin_feat], dim=1)\n",
        "        fused_feat = self.fusion_proj(fused)\n",
        "        fusion_out = self.fusion_head(fused_feat)\n",
        "\n",
        "        return img_out, clin_out, fusion_out"
      ],
      "metadata": {
        "id": "j0JV86QXs_xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Evaluation"
      ],
      "metadata": {
        "id": "fdlTrl9GvE9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(y_true, y_pred, y_prob, threshold):\n",
        "    \"\"\" 예측 결과(y_pred, y_prob)를 기반으로 모든 요구 지표를 계산합니다. \"\"\"\n",
        "\n",
        "    y_pred_thresh = (y_prob >= threshold).astype(int)\n",
        "\n",
        "    # Confusion Matrix 계산\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_thresh, labels=[0, 1]).ravel()\n",
        "\n",
        "    # 1. Accuracy\n",
        "    accuracy = accuracy_score(y_true, y_pred_thresh)\n",
        "    # 2. Recall (Sensitivity)\n",
        "    recall = recall_score(y_true, y_pred_thresh, zero_division=0)\n",
        "    # 3. Precision\n",
        "    precision = precision_score(y_true, y_pred_thresh, zero_division=0)\n",
        "    # 4. F1-score\n",
        "    f1 = f1_score(y_true, y_pred_thresh, zero_division=0)\n",
        "    # 5. ROC-AUC\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, y_prob)\n",
        "    except ValueError:\n",
        "        auc = np.nan\n",
        "    # 6. PR-AUC (Average Precision Score)\n",
        "    pr_auc = average_precision_score(y_true, y_prob)\n",
        "    # 7. Specificity (TNR)\n",
        "    # TNR = TN / (TN + FP)\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"Acc\": accuracy,\n",
        "        \"Recall (Sensitivity)\": recall,\n",
        "        \"Precision\": precision,\n",
        "        \"F1 Score\": f1,\n",
        "        \"ROC-AUC\": auc,\n",
        "        \"PR-AUC\": pr_auc,\n",
        "        \"Specificity (TNR)\": specificity,\n",
        "        \"Confusion Matrix\": (tn, fp, fn, tp)\n",
        "    }\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model_full(model, loader, device, threshold=0.5):\n",
        "    model.eval()\n",
        "    y_true, y_prob = [], []\n",
        "\n",
        "    for x_img, x_clin, y_img, y_clin in loader:\n",
        "        x_img, x_clin = x_img.to(device), x_clin.to(device)\n",
        "\n",
        "        _, _, fusion_out = model(x_img, x_clin)\n",
        "        probs = torch.sigmoid(fusion_out)\n",
        "\n",
        "        y_true.extend(y_img.cpu().numpy().ravel())\n",
        "        y_prob.extend(probs.cpu().numpy().ravel())\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_prob = np.array(y_prob)\n",
        "    y_pred = (y_prob >= threshold).astype(int) # 예측 결과 (임계값 적용)\n",
        "\n",
        "    metrics = calculate_metrics(y_true, y_pred, y_prob, threshold)\n",
        "\n",
        "    print(f\"\\n--- Evaluation at Threshold {threshold} ---\")\n",
        "    print(f\"Accuracy: {metrics['Acc']:.4f} | F1 Score: {metrics['F1 Score']:.4f} | ROC-AUC: {metrics['ROC-AUC']:.4f}\")\n",
        "    print(f\"Recall: {metrics['Recall (Sensitivity)']:.4f} | Precision: {metrics['Precision']:.4f} | Specificity (TNR): {metrics['Specificity (TNR)']:.4f}\")\n",
        "    print(f\"PR-AUC: {metrics['PR-AUC']:.4f}\")\n",
        "    print(\"\\nConfusion Matrix (TN, FP, FN, TP):\", metrics['Confusion Matrix'])\n",
        "    # Classification Report 출력 (세부 정보)\n",
        "    print(\"\\nClassification report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=['Benign (0)', 'Malignant (1)'], digits=4, zero_division=0))"
      ],
      "metadata": {
        "id": "Vu9MPvmqvBwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Main execution"
      ],
      "metadata": {
        "id": "0L9tKBIQtrM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading pre-processed data from NPZ files...\")\n",
        "\n",
        "# Clinical DL Data\n",
        "try:\n",
        "    clin_train_data = np.load(DL_TRAIN_NPZ)\n",
        "    clin_test_data = np.load(DL_TEST_NPZ)\n",
        "\n",
        "    X_clin_train = clin_train_data['X_train']\n",
        "    Y_clin_train = clin_train_data['Y_train']\n",
        "    X_clin_test = clin_test_data['X_test']\n",
        "    Y_clin_test = clin_test_data['Y_test']\n",
        "\n",
        "    CLINICAL_DATA_DIM = X_clin_train.shape[1]\n",
        "    print(f\"Clinical DL data loaded. Feature Dim: {CLINICAL_DATA_DIM}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"[ERROR] Clinical NPZ files not found. Run clinical_data_prep_split_fixed.py first!\")\n",
        "    exit()\n",
        "\n",
        "# Image Data\n",
        "train_dataset = MMOTUUnpairedDataset(TRAIN_IMG_NPZ, X_clin_train, Y_clin_train, device=device)\n",
        "test_dataset = MMOTUUnpairedDataset(TEST_IMG_NPZ, X_clin_test, Y_clin_test, device=device)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 4\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILnWOQ_0XWIy",
        "outputId": "6ccab7c5-29f9-41e2-9742-4c1d6bec4565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pre-processed data from NPZ files...\n",
            "Clinical DL data loaded. Feature Dim: 37\n",
            "Dataset Initialized. Image Samples: 1912, Clinical Samples: 160080\n",
            "Dataset Initialized. Image Samples: 469, Clinical Samples: 40020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_encoder = ImageEncoder(IMG_MODEL_PATH, device).to(device)\n",
        "\n",
        "multi_model = MultiModalClassifier_LateFusion(\n",
        "    img_encoder=image_encoder,\n",
        "    clinical_dim=CLINICAL_DATA_DIM\n",
        ").to(device)\n",
        "\n",
        "# Loss Weights & Optimizer 설정\n",
        "neg = (train_dataset.img_labels == 0).sum().item()\n",
        "pos = (train_dataset.img_labels == 1).sum().item()\n",
        "pos_weight_value = (neg / pos) * 2.5\n",
        "pos_weight = torch.tensor([pos_weight_value], device=device)\n",
        "\n",
        "criterion_img = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "criterion_clin = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "criterion_fusion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "λ_img, λ_clin, λ_fusion = 1.0, 1.0, 1.0\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, multi_model.parameters()),\n",
        "    lr=5e-5, weight_decay=0.01\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwBJKv3AXf2C",
        "outputId": "71d8a5c5-1713-4800-f237-c396e3fa0a34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 178MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Encoder loaded successfully from resnet50_binary_mmotu_aug.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 5\n",
        "print(f\"\\nStarting MultiModal Binary Training for {NUM_EPOCHS} epochs ...\")\n",
        "print(f\"Total Trainable Params: {sum(p.numel() for p in multi_model.parameters() if p.requires_grad) / 1e6:.2f} Million\")\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    multi_model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for x_img, x_clin, y_img, y_clin in tqdm(train_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\"):\n",
        "        x_img, x_clin = x_img.to(device), x_clin.to(device)\n",
        "        y_img = y_img.unsqueeze(1).to(device)\n",
        "        y_clin = y_clin.unsqueeze(1).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            img_out, clin_out, fusion_out = multi_model(x_img, x_clin)\n",
        "\n",
        "            loss_img = criterion_img(img_out, y_img)\n",
        "            loss_clin = criterion_clin(clin_out, y_clin)\n",
        "            loss_fusion = criterion_fusion(fusion_out, y_img)  # or y_clin\n",
        "\n",
        "            loss = (\n",
        "                λ_img * loss_img +\n",
        "                λ_clin * loss_clin +\n",
        "                λ_fusion * loss_fusion\n",
        "            )\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(f\"[Epoch {epoch:02d}] Time: {end_time - start_time:.1f}s | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(f\"\\nSaving final model state dict to {MULTIMODAL_MODEL_PATH}\")\n",
        "torch.save(multi_model.state_dict(), MULTIMODAL_MODEL_PATH)\n",
        "print(\"Final multimodal classifier saved successfully.\")\n",
        "\n",
        "print(\"\\n=== FINAL MULTIMODAL EVALUATION (Threshold Tuning) ===\")\n",
        "evaluate_model_full(multi_model, test_loader, device, threshold=0.5)\n",
        "evaluate_model_full(multi_model, test_loader, device, threshold=0.4)\n",
        "evaluate_model_full(multi_model, test_loader, device, threshold=0.3)\n",
        "evaluate_model_full(multi_model, test_loader, device, threshold=0.2)\n",
        "evaluate_model_full(multi_model, test_loader, device, threshold=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sRYGsgMXk_u",
        "outputId": "da7de51a-a9a5-404b-9635-36b9db64b620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting MultiModal Binary Training for 5 epochs ...\n",
            "Total Trainable Params: 1.16 Million\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 2501/2501 [03:54<00:00, 10.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 01] Time: 234.8s | Avg Loss: 1.8194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 2501/2501 [03:49<00:00, 10.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 02] Time: 229.7s | Avg Loss: 1.4031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 2501/2501 [03:50<00:00, 10.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 03] Time: 230.4s | Avg Loss: 1.2921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 2501/2501 [03:50<00:00, 10.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 04] Time: 230.4s | Avg Loss: 1.2350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 2501/2501 [03:50<00:00, 10.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 05] Time: 230.4s | Avg Loss: 1.1891\n",
            "\n",
            "Saving final model state dict to /content/drive/My Drive/졸업프로젝트/TestDataset/multimodal_classifier_latefusion.pth\n",
            "Final multimodal classifier saved successfully.\n",
            "\n",
            "=== FINAL MULTIMODAL EVALUATION (Threshold Tuning) ===\n",
            "\n",
            "--- Evaluation at Threshold 0.5 ---\n",
            "Accuracy: 0.9531 | F1 Score: 0.3538 | ROC-AUC: 0.7618\n",
            "Recall: 0.4009 | Precision: 0.3165 | Specificity (TNR): 0.9713\n",
            "PR-AUC: 0.1855\n",
            "\n",
            "Confusion Matrix (TN, FP, FN, TP): (np.int64(37628), np.int64(1110), np.int64(768), np.int64(514))\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   Benign (0)     0.9800    0.9713    0.9757     38738\n",
            "Malignant (1)     0.3165    0.4009    0.3538      1282\n",
            "\n",
            "     accuracy                         0.9531     40020\n",
            "    macro avg     0.6483    0.6861    0.6647     40020\n",
            " weighted avg     0.9587    0.9531    0.9557     40020\n",
            "\n"
          ]
        }
      ]
    }
  ]
}