{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Import"
      ],
      "metadata": {
        "id": "WZihchvPjE69"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjwP_dIVi9vR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "from torchvision.models import ResNet50_Weights\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, f1_score, recall_score,\n",
        "    precision_score, roc_auc_score, average_precision_score, accuracy_score\n",
        ")\n",
        "\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhIfwgdMjLK0",
        "outputId": "5cd584e4-040b-47d3-ef45-f94db73570da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "aqUH2iyujNAW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bca68de-f597-4886-8a11-8b1748cf14e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data & Model loading"
      ],
      "metadata": {
        "id": "v51oonVIjoJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = Path(\"/content/drive/My Drive/졸업프로젝트/TestDataset/\")\n",
        "IMG_MODEL_PATH = BASE_DIR / \"MMOTU/resnet50_binary_mmotu_aug.pth\"\n",
        "MULTIMODAL_MODEL_PATH = BASE_DIR / \"multimodal_classifier_v2.pth\"\n",
        "\n",
        "# Clinical Data\n",
        "PROCESSED_CLIN_DIR = Path(BASE_DIR) / \"Processed_Clinical_Data\"\n",
        "DL_TRAIN_NPZ = PROCESSED_CLIN_DIR / 'dl_train_data.npz'\n",
        "DL_TEST_NPZ = PROCESSED_CLIN_DIR / 'dl_test_data.npz'\n",
        "\n",
        "# US image data\n",
        "PROCESSED_IMG_DIR = BASE_DIR / \"MMOTU/ResNet50_ViT_Processed_Data\"\n",
        "TRAIN_IMG_NPZ = PROCESSED_IMG_DIR / \"train_augmented2_data.npz\"\n",
        "TEST_IMG_NPZ = PROCESSED_IMG_DIR / \"validation_data.npz\""
      ],
      "metadata": {
        "id": "pbKVogIsjOOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Preprocessing"
      ],
      "metadata": {
        "id": "A5yInRRHj3js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BINARY_MAPPING = {\n",
        "    0: 0, # Chocolate cyst (Benign)\n",
        "    1: 0, # Serous cystadenoma (Benign)\n",
        "    2: 0, # Teratoma (Benign)\n",
        "    3: 0, # Theca cell tumor (Benign)\n",
        "    4: 0, # Simple cyst (Benign)\n",
        "    5: 0, # Normal ovary (Benign)\n",
        "    6: 0, # Mucinous cystadenoma (Benign/Borderline)\n",
        "    7: 1  # High grade serous cystadenocarcinoma (Malignant)\n",
        "}"
      ],
      "metadata": {
        "id": "NhY5I08ljugb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MMOTUUnpairedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    초음파 이미지 데이터(.npz)와 임상 데이터(.npz)를 독립적으로 loading하고,\n",
        "    Co-training을 위한 random sampling 수행\n",
        "    \"\"\"\n",
        "    def __init__(self, img_npz_path, X_clin_np, Y_clin_np, device='cpu'):\n",
        "        img_data = np.load(img_npz_path, allow_pickle=True)\n",
        "        self.img_images = torch.from_numpy(img_data['images']).float()\n",
        "        self.img_labels = torch.from_numpy(img_data['labels'].astype(np.float32)).float()\n",
        "\n",
        "        self.clin_data = torch.from_numpy(X_clin_np).float()\n",
        "        self.clin_labels = torch.from_numpy(Y_clin_np).float()\n",
        "\n",
        "        self.img_len = len(self.img_images)\n",
        "        self.clin_len = len(self.clin_data)\n",
        "        self.max_len = max(self.img_len, self.clin_len)\n",
        "        print(f\"Dataset Initialized. Image Samples: {self.img_len}, Clinical Samples: {self.clin_len}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.max_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_idx = idx % self.img_len\n",
        "        clin_idx = random.randint(0, self.clin_len - 1)\n",
        "\n",
        "        return (\n",
        "            self.img_images[img_idx],\n",
        "            self.clin_data[clin_idx],\n",
        "            self.img_labels[img_idx],\n",
        "            self.clin_labels[clin_idx]\n",
        "        )"
      ],
      "metadata": {
        "id": "BGFsbW2AofH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Encoders"
      ],
      "metadata": {
        "id": "jasEC-GLtkrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, model_path, device):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        self.backbone = nn.Sequential(\n",
        "            resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool,\n",
        "            resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4,\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        self.output_dim = resnet.fc.in_features  # 2048\n",
        "\n",
        "        try:\n",
        "            state_dict = torch.load(model_path, map_location='cpu')\n",
        "            model_dict = self.backbone.state_dict()\n",
        "            pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
        "            model_dict.update(pretrained_dict)\n",
        "            self.backbone.load_state_dict(model_dict, strict=False)\n",
        "            print(f\"Image Encoder loaded successfully from {model_path.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN]: Failed to load pretrained Image Encoder. Training from scratch. Error: {e}\")\n",
        "\n",
        "        # fine tuning : layer 3, 4만 학습 가능하도록 설정\n",
        "        for name, param in self.backbone.named_parameters():\n",
        "             if 'layer4' in name or 'layer3' in name:\n",
        "                 param.requires_grad = True\n",
        "             else:\n",
        "                 param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.flatten(self.backbone(x), 1)"
      ],
      "metadata": {
        "id": "PfA8VaYisw4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClinicalEncoder(nn.Module):\n",
        "    def __init__(self, clinical_dim, shared_dim=512, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(clinical_dim, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, shared_dim) # Shared Backbone 입력 차원\n",
        "        )\n",
        "        self.output_dim = shared_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)"
      ],
      "metadata": {
        "id": "kq_C_wmPVgoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiModalClassifier_UML(nn.Module):\n",
        "    def __init__(self, img_encoder, clinical_dim, shared_dim=512, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        # Image Encoder\n",
        "        self.image_encoder = img_encoder\n",
        "        self.img_proj = nn.Linear(self.image_encoder.output_dim, shared_dim)\n",
        "\n",
        "        # Clinical Encoder\n",
        "        self.clinical_encoder = ClinicalEncoder(clinical_dim, shared_dim, dropout_rate)\n",
        "\n",
        "        # 1. Shared Backbone\n",
        "        self.shared_backbone = nn.Sequential(\n",
        "            nn.Linear(shared_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.shared_output_dim = 128\n",
        "\n",
        "        # 2. Shared Classifier\n",
        "        self.shared_classifier = nn.Sequential(\n",
        "            nn.Linear(self.shared_output_dim, 1) # Binary Classification\n",
        "        )\n",
        "\n",
        "    def forward_single_modality(self, x_modality, modality_type):\n",
        "        \"\"\"단일 모달리티의 입력을 받아 Shared Backbone을 통과 후 Logit 반환\"\"\"\n",
        "\n",
        "        if modality_type == 'image':\n",
        "            feat = self.image_encoder(x_modality)\n",
        "            feat_proj = self.img_proj(feat)\n",
        "        elif modality_type == 'clinical':\n",
        "            feat_proj = self.clinical_encoder(x_modality)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid modality type\")\n",
        "\n",
        "        # Shared Backbone 통과 (Cross-modal transfer 발생)\n",
        "        shared_feat = self.shared_backbone(feat_proj)\n",
        "\n",
        "        # Shared Classifier 통과\n",
        "        logits = self.shared_classifier(shared_feat)\n",
        "        return logits, shared_feat\n",
        "\n",
        "    def forward(self, x_img, x_clin):\n",
        "        logits_img, _ = self.forward_single_modality(x_img, 'image')\n",
        "        logits_clin, _ = self.forward_single_modality(x_clin, 'clinical')\n",
        "\n",
        "        return logits_img, logits_clin"
      ],
      "metadata": {
        "id": "j0JV86QXs_xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Evaluation"
      ],
      "metadata": {
        "id": "fdlTrl9GvE9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(y_true, y_pred, y_prob, threshold):\n",
        "    \"\"\" 예측 결과(y_pred, y_prob)를 기반으로 모든 요구 지표를 계산합니다. \"\"\"\n",
        "\n",
        "    y_pred_thresh = (y_prob >= threshold).astype(int)\n",
        "\n",
        "    # Confusion Matrix 계산\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_thresh, labels=[0, 1]).ravel()\n",
        "\n",
        "    # 1. Accuracy\n",
        "    accuracy = accuracy_score(y_true, y_pred_thresh)\n",
        "    # 2. Recall (Sensitivity)\n",
        "    recall = recall_score(y_true, y_pred_thresh, zero_division=0)\n",
        "    # 3. Precision\n",
        "    precision = precision_score(y_true, y_pred_thresh, zero_division=0)\n",
        "    # 4. F1-score\n",
        "    f1 = f1_score(y_true, y_pred_thresh, zero_division=0)\n",
        "    # 5. ROC-AUC\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, y_prob)\n",
        "    except ValueError:\n",
        "        auc = np.nan\n",
        "    # 6. PR-AUC (Average Precision Score)\n",
        "    pr_auc = average_precision_score(y_true, y_prob)\n",
        "    # 7. Specificity (TNR)\n",
        "    # TNR = TN / (TN + FP)\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"Acc\": accuracy,\n",
        "        \"Recall (Sensitivity)\": recall,\n",
        "        \"Precision\": precision,\n",
        "        \"F1 Score\": f1,\n",
        "        \"ROC-AUC\": auc,\n",
        "        \"PR-AUC\": pr_auc,\n",
        "        \"Specificity (TNR)\": specificity,\n",
        "        \"Confusion Matrix\": (tn, fp, fn, tp)\n",
        "    }\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model_full(model, loader, device, threshold=0.5):\n",
        "    model.eval()\n",
        "    y_true, y_prob = [], []\n",
        "\n",
        "    for x_img, x_clin, y_img, y_clin in loader:\n",
        "        x_img = x_img.to(device)\n",
        "\n",
        "        # 평가 시에는 이미지 모달리티만 사용하여 예측\n",
        "        logits, _ = model.forward_single_modality(x_img, 'image')\n",
        "        probs = torch.sigmoid(logits)\n",
        "\n",
        "        y_true.extend(y_img.cpu().numpy().ravel())\n",
        "        y_prob.extend(probs.cpu().numpy().ravel())\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_prob = np.array(y_prob)\n",
        "    y_pred = (y_prob >= threshold).astype(int) # 예측 결과 (임계값 적용)\n",
        "\n",
        "    metrics = calculate_metrics(y_true, y_pred, y_prob, threshold)\n",
        "\n",
        "    print(f\"\\n--- Evaluation at Threshold {threshold} ---\")\n",
        "    print(f\"Accuracy: {metrics['Acc']:.4f} | F1 Score: {metrics['F1 Score']:.4f} | ROC-AUC: {metrics['ROC-AUC']:.4f}\")\n",
        "    print(f\"Recall: {metrics['Recall (Sensitivity)']:.4f} | Precision: {metrics['Precision']:.4f} | Specificity (TNR): {metrics['Specificity (TNR)']:.4f}\")\n",
        "    print(f\"PR-AUC: {metrics['PR-AUC']:.4f}\")\n",
        "    print(\"\\nConfusion Matrix (TN, FP, FN, TP):\", metrics['Confusion Matrix'])\n",
        "    # Classification Report 출력 (세부 정보)\n",
        "    print(\"\\nClassification report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=['Benign (0)', 'Malignant (1)'], digits=4, zero_division=0))"
      ],
      "metadata": {
        "id": "Vu9MPvmqvBwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Main execution"
      ],
      "metadata": {
        "id": "0L9tKBIQtrM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading pre-processed data from NPZ files...\")\n",
        "\n",
        "# Clinical DL Data\n",
        "try:\n",
        "    clin_train_data = np.load(DL_TRAIN_NPZ)\n",
        "    clin_test_data = np.load(DL_TEST_NPZ)\n",
        "\n",
        "    X_clin_train = clin_train_data['X_train']\n",
        "    Y_clin_train = clin_train_data['Y_train']\n",
        "    X_clin_test = clin_test_data['X_test']\n",
        "    Y_clin_test = clin_test_data['Y_test']\n",
        "\n",
        "    CLINICAL_DATA_DIM = X_clin_train.shape[1]\n",
        "    print(f\"Clinical DL data loaded. Feature Dim: {CLINICAL_DATA_DIM}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"[ERROR] Clinical NPZ files not found. Run clinical_data_prep_split_fixed.py first!\")\n",
        "    exit()\n",
        "\n",
        "# Image Data\n",
        "train_dataset = MMOTUUnpairedDataset(TRAIN_IMG_NPZ, X_clin_train, Y_clin_train, device=device)\n",
        "test_dataset = MMOTUUnpairedDataset(TEST_IMG_NPZ, X_clin_test, Y_clin_test, device=device)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 4\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILnWOQ_0XWIy",
        "outputId": "03233c79-fe51-4736-869f-c82cc2276fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pre-processed data from NPZ files...\n",
            "Clinical DL data loaded. Feature Dim: 37\n",
            "Dataset Initialized. Image Samples: 1912, Clinical Samples: 160080\n",
            "Dataset Initialized. Image Samples: 469, Clinical Samples: 40020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.75, gamma=2):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        bce = F.binary_cross_entropy_with_logits(\n",
        "            logits, targets, reduction='none'\n",
        "        )\n",
        "        pt = torch.exp(-bce)\n",
        "        loss = self.alpha * (1 - pt)**self.gamma * bce\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "WO3t0Gb3rD1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_encoder = ImageEncoder(IMG_MODEL_PATH, device).to(device)\n",
        "\n",
        "multi_model = MultiModalClassifier_UML(\n",
        "    img_encoder=image_encoder,\n",
        "    clinical_dim=CLINICAL_DATA_DIM\n",
        ").to(device)\n",
        "\n",
        "# Loss Weights & Optimizer 설정\n",
        "neg_img = (train_dataset.img_labels == 0).sum().item()\n",
        "pos_img = (train_dataset.img_labels == 1).sum().item()\n",
        "\n",
        "neg_clin = (train_dataset.clin_labels == 0).sum().item()\n",
        "pos_clin = (train_dataset.clin_labels == 1).sum().item()\n",
        "\n",
        "neg = neg_img + neg_clin\n",
        "pos = pos_img + pos_clin\n",
        "\n",
        "pos_weight_value = (neg / pos) * 3.0\n",
        "pos_weight = torch.tensor([pos_weight_value], device=device)\n",
        "\n",
        "# criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer = optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, multi_model.parameters()),\n",
        "    lr=5e-5, weight_decay=0.01\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwBJKv3AXf2C",
        "outputId": "9b8a33d5-cd1b-438b-8616-36fcafe634b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Encoder loaded successfully from resnet50_binary_mmotu_aug.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 5\n",
        "print(f\"\\nStarting MultiModal Binary Training for {NUM_EPOCHS} epochs ...\")\n",
        "print(f\"Total Trainable Params: {sum(p.numel() for p in multi_model.parameters() if p.requires_grad) / 1e6:.2f} Million\")\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    multi_model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for x_img, x_clin, y_img, y_clin in tqdm(train_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\"):\n",
        "\n",
        "        x_img = x_img.to(device)\n",
        "        x_clin = x_clin.to(device)\n",
        "        y_img = y_img.to(device).unsqueeze(1)\n",
        "        y_clin = y_clin.to(device).unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            logits_img, logits_clin = multi_model(x_img, x_clin)\n",
        "\n",
        "            L_img_cls = criterion(logits_img, y_img)\n",
        "\n",
        "            L_clin_cls = criterion(logits_clin, y_clin)\n",
        "\n",
        "            # Total Loss: 이 손실이 Shared Backbone을 업데이트\n",
        "            total_batch_loss = L_img_cls + L_clin_cls\n",
        "\n",
        "        scaler.scale(total_batch_loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += total_batch_loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(f\"[Epoch {epoch:02d}] Time: {end_time - start_time:.1f}s | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(f\"\\nSaving final model state dict to {MULTIMODAL_MODEL_PATH}\")\n",
        "torch.save(multi_model.state_dict(), MULTIMODAL_MODEL_PATH)\n",
        "print(\"Final multimodal classifier saved successfully.\")\n",
        "\n",
        "print(\"\\n=== FINAL MULTIMODAL EVALUATION ===\")\n",
        "evaluate_model_full(multi_model, test_loader, device, threshold=0.5)\n",
        "evaluate_model_full(multi_model, test_loader, device, threshold=0.4)\n",
        "evaluate_model_full(multi_model, test_loader, device, threshold=0.3)\n",
        "evaluate_model_full(multi_model, test_loader, device, threshold=0.2)\n",
        "evaluate_model_full(multi_model, test_loader, device, threshold=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sRYGsgMXk_u",
        "outputId": "c21f055a-b02b-4127-9b62-054e7e4b4ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting MultiModal Binary Training for 5 epochs ...\n",
            "Total Trainable Params: 2.04 Million\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 2501/2501 [03:53<00:00, 10.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 01] Time: 233.2s | Avg Loss: 2.2538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 2501/2501 [03:52<00:00, 10.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 02] Time: 232.4s | Avg Loss: 1.9161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 2501/2501 [03:52<00:00, 10.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 03] Time: 232.5s | Avg Loss: 1.8660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 2501/2501 [03:52<00:00, 10.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 04] Time: 232.8s | Avg Loss: 1.8522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 2501/2501 [03:52<00:00, 10.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 05] Time: 232.6s | Avg Loss: 1.8603\n",
            "\n",
            "Saving final model state dict to /content/drive/My Drive/졸업프로젝트/TestDataset/multimodal_classifier_v2.pth\n",
            "Final multimodal classifier saved successfully.\n",
            "\n",
            "=== FINAL MULTIMODAL EVALUATION ===\n",
            "\n",
            "--- Evaluation at Threshold 0.5 ---\n",
            "Accuracy: 0.9595 | F1 Score: 0.3881 | ROC-AUC: 0.7449\n",
            "Recall: 0.4009 | Precision: 0.3760 | Specificity (TNR): 0.9780\n",
            "PR-AUC: 0.2215\n",
            "\n",
            "Confusion Matrix (TN, FP, FN, TP): (np.int64(37885), np.int64(853), np.int64(768), np.int64(514))\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   Benign (0)     0.9801    0.9780    0.9791     38738\n",
            "Malignant (1)     0.3760    0.4009    0.3881      1282\n",
            "\n",
            "     accuracy                         0.9595     40020\n",
            "    macro avg     0.6781    0.6895    0.6836     40020\n",
            " weighted avg     0.9608    0.9595    0.9601     40020\n",
            "\n",
            "\n",
            "--- Evaluation at Threshold 0.4 ---\n",
            "Accuracy: 0.9509 | F1 Score: 0.3437 | ROC-AUC: 0.7449\n",
            "Recall: 0.4009 | Precision: 0.3008 | Specificity (TNR): 0.9692\n",
            "PR-AUC: 0.2215\n",
            "\n",
            "Confusion Matrix (TN, FP, FN, TP): (np.int64(37543), np.int64(1195), np.int64(768), np.int64(514))\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   Benign (0)     0.9800    0.9692    0.9745     38738\n",
            "Malignant (1)     0.3008    0.4009    0.3437      1282\n",
            "\n",
            "     accuracy                         0.9509     40020\n",
            "    macro avg     0.6404    0.6850    0.6591     40020\n",
            " weighted avg     0.9582    0.9509    0.9543     40020\n",
            "\n",
            "\n",
            "--- Evaluation at Threshold 0.3 ---\n",
            "Accuracy: 0.9488 | F1 Score: 0.3341 | ROC-AUC: 0.7449\n",
            "Recall: 0.4009 | Precision: 0.2864 | Specificity (TNR): 0.9669\n",
            "PR-AUC: 0.2215\n",
            "\n",
            "Confusion Matrix (TN, FP, FN, TP): (np.int64(37457), np.int64(1281), np.int64(768), np.int64(514))\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   Benign (0)     0.9799    0.9669    0.9734     38738\n",
            "Malignant (1)     0.2864    0.4009    0.3341      1282\n",
            "\n",
            "     accuracy                         0.9488     40020\n",
            "    macro avg     0.6331    0.6839    0.6537     40020\n",
            " weighted avg     0.9577    0.9488    0.9529     40020\n",
            "\n",
            "\n",
            "--- Evaluation at Threshold 0.2 ---\n",
            "Accuracy: 0.9424 | F1 Score: 0.3084 | ROC-AUC: 0.7449\n",
            "Recall: 0.4009 | Precision: 0.2506 | Specificity (TNR): 0.9603\n",
            "PR-AUC: 0.2215\n",
            "\n",
            "Confusion Matrix (TN, FP, FN, TP): (np.int64(37201), np.int64(1537), np.int64(768), np.int64(514))\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   Benign (0)     0.9798    0.9603    0.9700     38738\n",
            "Malignant (1)     0.2506    0.4009    0.3084      1282\n",
            "\n",
            "     accuracy                         0.9424     40020\n",
            "    macro avg     0.6152    0.6806    0.6392     40020\n",
            " weighted avg     0.9564    0.9424    0.9488     40020\n",
            "\n",
            "\n",
            "--- Evaluation at Threshold 0.1 ---\n",
            "Accuracy: 0.9275 | F1 Score: 0.2615 | ROC-AUC: 0.7449\n",
            "Recall: 0.4009 | Precision: 0.1940 | Specificity (TNR): 0.9449\n",
            "PR-AUC: 0.2215\n",
            "\n",
            "Confusion Matrix (TN, FP, FN, TP): (np.int64(36603), np.int64(2135), np.int64(768), np.int64(514))\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   Benign (0)     0.9794    0.9449    0.9619     38738\n",
            "Malignant (1)     0.1940    0.4009    0.2615      1282\n",
            "\n",
            "     accuracy                         0.9275     40020\n",
            "    macro avg     0.5867    0.6729    0.6117     40020\n",
            " weighted avg     0.9543    0.9275    0.9394     40020\n",
            "\n"
          ]
        }
      ]
    }
  ]
}